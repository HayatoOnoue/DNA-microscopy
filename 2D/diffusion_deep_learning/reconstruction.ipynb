{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXEnmFmUpHvj"
   },
   "source": [
    "# GCNNによって隣接行列からノードの座標の予測する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhVO0ABfv1r1"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "from natsort import natsorted\n",
    "from scipy.io import mmread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data, DataLoader, Dataset, InMemoryDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_scatter import scatter\n",
    "\n",
    "# random seed\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "use_InMemoryDataset = True\n",
    "add_noise = True\n",
    "use_graph_distance = True\n",
    "drop_probability = 0.01\n",
    "mean, sigma = 0, 1\n",
    "pow_ = -1.0\n",
    "K, L0 = 1.0, 1.0\n",
    "EPS = 1e-6\n",
    "\n",
    "n_jobs = int(os.cpu_count() * 0.8)\n",
    "\n",
    "data_size_type = \"small_\" if test_run else \"large_\"\n",
    "Dataset_type = \"InMemoryDataset\" if use_InMemoryDataset else \"Dataset\"\n",
    "root = osp.join(\"data\", data_size_type + \"reconstruction_\" + Dataset_type)\n",
    "\n",
    "if test_run:\n",
    "    epoch_num = 5\n",
    "    batch_size = 3\n",
    "else:\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "\n",
    "print(\"n_jobs = {}\".format(n_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataオブジェクトを作成する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nxG(adj_coo_mats, coords_ndas, i):\n",
    "    num_beads = int(np.sqrt(adj_coo_mats.shape[1]))\n",
    "    # convert each row of raw data to adj\n",
    "    adj_coo = adj_coo_mats.getrow(i).reshape(num_beads, num_beads)\n",
    "    # convert each row of raw data to coords\n",
    "    coords_nda = coords_ndas[i, :].reshape(num_beads, -1)\n",
    "    x = 1.0\n",
    "    pos = dict(zip(range(num_beads), coords_nda))\n",
    "\n",
    "    nxG = nx.from_scipy_sparse_matrix(adj_coo)\n",
    "    nx.set_node_attributes(nxG, x, \"x\")\n",
    "    nx.set_node_attributes(nxG, pos, \"pos\")\n",
    "    return nxG\n",
    "\n",
    "\n",
    "def drop_edge(nxG, prob=0.01):\n",
    "    num_edges = nxG.number_of_edges()\n",
    "    mask = np.where(np.random.rand(num_edges) > prob, True, False)\n",
    "    ebunch = np.array(nxG.edges)[~mask]\n",
    "    nxG.remove_edges_from(ebunch)\n",
    "    return nxG\n",
    "\n",
    "\n",
    "def multiply_lognormal_noise(nxG, mean=0, sigma=1):\n",
    "    size = nxG.number_of_edges()\n",
    "    lognormals = np.random.lognormal(mean=mean, sigma=sigma, size=size)\n",
    "    ew = nx.get_edge_attributes(nxG, \"weight\")\n",
    "    e = list(ew.keys())\n",
    "    w = np.array(list(ew.values()))\n",
    "    w = w * lognormals\n",
    "    ew = dict(zip(e, w))\n",
    "    nx.set_edge_attributes(nxG, ew, \"weight\")\n",
    "    return nxG\n",
    "\n",
    "\n",
    "def invert_edge_attr(nxG, pow_):\n",
    "    ew = nx.get_edge_attributes(nxG, \"weight\")\n",
    "    e = list(ew.keys())\n",
    "    w = np.array(list(ew.values()))\n",
    "    w = w ** pow_\n",
    "    ew = dict(zip(e, w))\n",
    "    nx.set_edge_attributes(nxG, ew, \"weight\")\n",
    "    return nxG\n",
    "\n",
    "\n",
    "def nxG_to_Data(nxG, use_graph_distance=False):\n",
    "    num_nodes = nxG.number_of_nodes()\n",
    "\n",
    "    x = torch.tensor(\n",
    "        np.array(list(nx.get_node_attributes(nxG, \"x\").values())).reshape(\n",
    "            num_nodes, -1\n",
    "        ),\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "    pos = torch.tensor(\n",
    "        list(nx.get_node_attributes(nxG, \"pos\").values()), dtype=torch.float\n",
    "    )\n",
    "\n",
    "    nxG = nx.to_directed(nxG)  # To represent Graph with coo format\n",
    "    num_edges = nxG.number_of_edges()\n",
    "\n",
    "    edge_index = torch.tensor(np.array(nxG.edges).T, dtype=torch.long)\n",
    "    edge_attr = torch.tensor(\n",
    "        np.array(list(nx.get_edge_attributes(nxG, \"weight\").values())).reshape(\n",
    "            num_edges, -1\n",
    "        ),\n",
    "        dtype=torch.float,\n",
    "    )\n",
    "\n",
    "    if use_graph_distance:\n",
    "        graph_dist = torch.full((num_nodes, num_nodes), np.inf).float()\n",
    "        dict_graph_dist = dict(nx.shortest_path_length(nxG, weight=\"weight\"))\n",
    "        for i in range(num_nodes):\n",
    "            for j, d in dict_graph_dist[i].items():\n",
    "                graph_dist[i][j] = d\n",
    "        graph_dist = graph_dist.view(-1, 1)  # to make the dim1 the same size\n",
    "        data = Data(\n",
    "            x=x,\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            pos=pos,\n",
    "            graph_dist=graph_dist,\n",
    "        )\n",
    "    else:\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, pos=pos)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2O2r6c7vFPo"
   },
   "source": [
    "## ネットワーク, Lossの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # TODO: 入力グラフのチャネル数への依存をなくす\n",
    "        self.conv1 = GCNConv(1, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.conv3 = GCNConv(32, 48)\n",
    "        self.conv4 = GCNConv(48, 64)\n",
    "        self.conv5 = GCNConv(64, 96)\n",
    "        self.conv6 = GCNConv(96, 128)\n",
    "        self.linear1 = torch.nn.Linear(128, 64)\n",
    "        # TODO: 分類数への依存をなくす\n",
    "        self.linear2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # edge_weightとして用いるedge_attrのshapeが(n,1)だとうまくいかない．\n",
    "        # edge_attr.shapeが(n,)だと動く．おそらくPyG側のバグ？\n",
    "        edge_weight = torch.squeeze(data.edge_attr)\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class KKLoss(nn.Module):\n",
    "    \"\"\"Return energy of Kamada-Kawai as loss\"\"\"\n",
    "\n",
    "    def __init__(self, K=1.0, L0=1.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.L0 = L0\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, batch, prediction):\n",
    "        \"\"\"\n",
    "        The Kamada-Kawai loss of the graph included in \n",
    "        the batch is calculated in parallel as in the mini-batch.\n",
    "        \"\"\"\n",
    "        k = torch.zeros(batch.num_nodes, batch.num_nodes)\n",
    "        l = torch.zeros_like(k)\n",
    "        data_list = batch.to_data_list()\n",
    "        s = 0\n",
    "\n",
    "        for data in data_list:\n",
    "            num_nodes = data.num_nodes\n",
    "            d = data.graph_dist.view(num_nodes, num_nodes)\n",
    "            d_max = torch.unique(d, sorted=True)[-2]  # avoid inf\n",
    "            L = self.L0 / d_max\n",
    "            k[s : s + num_nodes, s : s + num_nodes] = self.K * torch.where(\n",
    "                d != 0, d ** -2, d\n",
    "            )\n",
    "            l[s : s + num_nodes, s : s + num_nodes] = L * d\n",
    "            l[l==float(\"inf\")] = 0  # avoid 0 * inf = nan\n",
    "            s += num_nodes\n",
    "\n",
    "        x = prediction[:, [0]] - prediction[:, 0]\n",
    "        y = prediction[:, [1]] - prediction[:, 1]\n",
    "        e = 0.5 * torch.sum(\n",
    "            0.5 * k * (x ** 2 + y ** 2 + l ** 2 - 2 * l * torch.sqrt(x ** 2 + y ** 2 + self.eps))\n",
    "        )\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "adj_path = osp.join(root, \"raw\", \"adjMats.mtx\")\n",
    "coords_path = osp.join(root, \"raw\", \"coords.mtx\")\n",
    "\n",
    "print(\"Reading graphs from {}\".format(root))\n",
    "\n",
    "adj_coo_mats = mmread(adj_path)\n",
    "coords_ndas = mmread(coords_path)\n",
    "num_samples = adj_coo_mats.shape[0]\n",
    "data_list = []\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "print(\"Finish reading graphs from storage.\")\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))\n",
    "print(\"num_samples: {}\".format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "print(\"Generating nx graph object\")\n",
    "\n",
    "nxG_list = Parallel(n_jobs=n_jobs)(\n",
    "    [\n",
    "        delayed(generate_nxG)(adj_coo_mats=adj_coo_mats, coords_ndas=coords_ndas, i=i)\n",
    "        for i in range(num_samples)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Post-processing\")\n",
    "\n",
    "if add_noise:\n",
    "    print(\"Dropping edges\")\n",
    "\n",
    "    data_list = Parallel(n_jobs=n_jobs)(\n",
    "        [delayed(drop_edge)(nxG=nxG, prob=drop_probability) for nxG in nxG_list]\n",
    "    )\n",
    "\n",
    "    print(\"Adding noise\")\n",
    "\n",
    "    data_list = Parallel(n_jobs=n_jobs)(\n",
    "        [\n",
    "            delayed(multiply_lognormal_noise)(nxG=nxG, mean=mean, sigma=sigma)\n",
    "            for nxG in nxG_list\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"Inverting edge_attr\")\n",
    "\n",
    "nxG_list = Parallel(n_jobs=n_jobs)(\n",
    "    [delayed(invert_edge_attr)(nxG=nxG, pow_=pow_) for nxG in nxG_list]\n",
    ")\n",
    "\n",
    "print(\"Finish post-processing!\")\n",
    "print(\"Converting to Data object\")\n",
    "\n",
    "data_list = Parallel(n_jobs=n_jobs)(\n",
    "    [\n",
    "        delayed(nxG_to_Data)(nxG, use_graph_distance=use_graph_distance)\n",
    "        for nxG in nxG_list\n",
    "    ]\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(\"Finish generating Data objects\")\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "val_size = round(num_samples * val_ratio)\n",
    "test_size = round(num_samples * test_ratio)\n",
    "train_size = num_samples - val_size - test_size\n",
    "cums = np.array([train_size, val_size, test_size]).cumsum()\n",
    "random.shuffle(data_list)\n",
    "\n",
    "train_data = data_list[0 : cums[0]]\n",
    "val_data = data_list[cums[0] : cums[1]]\n",
    "test_data = data_list[cums[1] : cums[2]]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "nBeads = train_data[0].num_nodes\n",
    "nDim = train_data[0].pos.shape[1]  ## 各頂点の座標の次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0RaAOKXP5MK"
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 161331,
     "status": "ok",
     "timestamp": 1594717726701,
     "user": {
      "displayName": "Hayato Onoue",
      "photoUrl": "",
      "userId": "07486968416296706504"
     },
     "user_tz": -540
    },
    "id": "EZkA2ZvhWpG1",
    "outputId": "76d9c5bf-cb6d-447b-c6b9-d0dfde5471d8"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = KKLoss(K=K, L0=L0, eps=EPS)\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    train_loss = 0.0\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(batch)\n",
    "        #loss = criterion(prediction, batch.pos)\n",
    "        loss = criterion(batch, prediction)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.cpu().item() * nDim\n",
    "        \n",
    "        if i % 10 == 9:\n",
    "            progress_bar = (\n",
    "                \"[\"\n",
    "                + (\"=\" * ((i + 1) // 10))\n",
    "                + (\" \" * ((train_size // 100 - (i + 1)) // 10))\n",
    "                + \"]\"\n",
    "            )\n",
    "            print(\n",
    "                \"\\repoch: {:d} loss: {:.3f}  {}\".format(\n",
    "                    epoch + 1,\n",
    "                    loss.cpu().item() * nDim,  # 表示するlossはノード間の距離の二条の平均\n",
    "                    progress_bar,\n",
    "                ),\n",
    "                end=\"  \",\n",
    "            )\n",
    "\n",
    "    print(\n",
    "        \"\\repoch: {:d} loss: {:.3f}\".format(\n",
    "            epoch + 1, train_loss / math.ceil(train_size / batch_size)\n",
    "        ),\n",
    "        end=\"  \",\n",
    "    )\n",
    "    history[\"train_loss\"].append(train_loss / math.ceil(train_size / batch_size))\n",
    "\n",
    "    batch_num = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            prediction = model(data)\n",
    "            #loss += criterion(prediction, data.pos) * nDim\n",
    "            loss += criterion(data, prediction)\n",
    "            batch_num += 1\n",
    "\n",
    "    history[\"val_loss\"].append(loss.cpu().item() / batch_num)\n",
    "    endstr = \" \" * max(1, (train_size // 1000 - 39)) + \"\\n\"\n",
    "    print(f\"Val Loss: {loss.cpu().item()/batch_num:.3f}\", end=endstr)\n",
    "\n",
    "\n",
    "print(\"Finished Training\")\n",
    "elapsed_time = time.time() - start\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBkC2aF4eGH-"
   },
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1712,
     "status": "ok",
     "timestamp": 1594718171173,
     "user": {
      "displayName": "Hayato Onoue",
      "photoUrl": "",
      "userId": "07486968416296706504"
     },
     "user_tz": -540
    },
    "id": "VN29VyfWeIh3",
    "outputId": "fa64a6c9-ae06-4683-afb0-d396c0935111"
   },
   "outputs": [],
   "source": [
    "# 損失\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "\n",
    "x = np.arange(epoch_num) + 1\n",
    "plt.plot(x, history[\"train_loss\"], label=\"train loss\")\n",
    "plt.plot(x, history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDcfJAmtFP4u"
   },
   "outputs": [],
   "source": [
    "# 頂点の描画\n",
    "t_index = 0\n",
    "node_list = list(range(nBeads))\n",
    "test_data = test_data[t_index].to(device)\n",
    "\n",
    "# テストデータのノードの座標を予想\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    estimated_coords = model(test_data)\n",
    "\n",
    "# 描画のためのグラフを作成\n",
    "test_edge_indices = torch.t(test_data.edge_index).to(\"cpu\").detach().numpy()\n",
    "true_coords = test_data.pos.to(\"cpu\").detach().numpy().copy()\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_edges_from(test_edge_indices)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.set_title(\"True\")\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.set_title(\"Estimated\")\n",
    "\n",
    "true_pos = dict(zip(node_list, true_coords))\n",
    "# nx.draw_networkx(G, pos=true_pos, with_labels=False, ax=ax1,\n",
    "#                 node_color=\"red\", node_size=2)\n",
    "nx.draw_networkx_nodes(G, pos=true_pos, ax=ax1, node_color=\"red\", node_size=2)\n",
    "\n",
    "estimated_pos = dict(zip(node_list, estimated_coords.cpu().detach().numpy()))\n",
    "# nx.draw_networkx(G, pos=estimated_pos, with_labels=False, ax=ax2,\n",
    "#                 node_color=\"red\", node_size=2)\n",
    "nx.draw_networkx_nodes(G, pos=estimated_pos, ax=ax2, node_color=\"red\", node_size=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 20\n",
    "\n",
    "print(\"=====Simulation conditions=====\")\n",
    "print(\"目的：隣接町列を生成した数字の予測\")\n",
    "print(\"ネットワーク：GCNN\")\n",
    "print(\"Test run: {}\".format(test_run))\n",
    "print(\"Add noise: {}\".format(add_noise))\n",
    "if add_noise:\n",
    "    print(\"Probabirity of edge drop: {}\".format(drop_probability))\n",
    "    print(\"Info about normal distribution: mean: {}, sigma: {}\".format(mean, sigma))\n",
    "    print(\"Pow: {}\".format(pow_))\n",
    "print(\"Number of beads: {}\".format(nBeads))\n",
    "print(\"Number of samples for training: {}\".format(train_size))\n",
    "print(\"Add noise: {}\".format(add_noise))\n",
    "print(\"Number of epochs: {}\".format(epoch_num))\n",
    "print(\"Batch size: {}\".format(batch_size))\n",
    "print(\"Diffusion time: {}\".format(t))\n",
    "\n",
    "print(\"=====Results=====\")\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "vgRFgRRheDVI"
   ],
   "name": "adj2coords_GCN_PyG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
