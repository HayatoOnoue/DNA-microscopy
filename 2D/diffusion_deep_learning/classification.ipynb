{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXEnmFmUpHvj"
   },
   "source": [
    "# GCNNによる隣接行列の分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリのインポート，変数の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PhVO0ABfv1r1"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import os.path as osp\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import mmread\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "from natsort import natsorted\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, Dataset, InMemoryDataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import Compose\n",
    "from torch_scatter import scatter\n",
    "\n",
    "# random seed\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run = True\n",
    "use_InMemoryDataset = True\n",
    "add_noise = True\n",
    "drop_probability = 0.01\n",
    "mean, sigma = 0, 1\n",
    "pow_ = -1.0\n",
    "\n",
    "n_jobs = int(os.cpu_count() * 0.8)\n",
    "\n",
    "data_size_type = \"small_\" if test_run else \"large_\"\n",
    "Dataset_type = \"InMemoryDataset\" if use_InMemoryDataset else \"Dataset\"\n",
    "root = osp.join(\"data\", data_size_type + \"classification_\" + Dataset_type)\n",
    "\n",
    "if test_run:\n",
    "    epoch_num = 3\n",
    "    batch_size = 128\n",
    "else:\n",
    "    epoch_num = 100\n",
    "    batch_size = 128\n",
    "\n",
    "print(\"n_jobs = {}\".format(n_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataオブジェクトを作成する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_edge(data, prob=0.01):\n",
    "    mask = np.where(np.random.rand(int(data.num_edges / 2)) > prob, True, False)\n",
    "    mask = np.concatenate((mask, mask), axis=0)\n",
    "    data.edge_index = data.edge_index[:, mask]\n",
    "    data.edge_attr = data.edge_attr[mask]\n",
    "    return data\n",
    "\n",
    "\n",
    "def multiply_lognormal_noise(data, mean=0, sigma=1):\n",
    "    size = np.array(data.edge_attr.shape)\n",
    "    size[0] /= 2\n",
    "    lognormals = torch.tensor(\n",
    "        np.random.lognormal(mean=mean, sigma=sigma, size=size)\n",
    "    ).float()\n",
    "    data.edge_attr = data.edge_attr * torch.cat((lognormals, lognormals), 0)\n",
    "    return data\n",
    "\n",
    "\n",
    "def invert_edge_attr(data, pow_):\n",
    "    data.edge_attr = data.edge_attr ** pow_\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_graphs(adj_coo_mats, classes, i):\n",
    "    num_beads = int(np.sqrt(adj_coo_mats.shape[1]))\n",
    "    # convert each row of raw data to adj\n",
    "    adj_coo = adj_coo_mats.getrow(i).reshape(num_beads, num_beads)\n",
    "    # convert each row of raw data to classes\n",
    "    class_ = classes[i, 0]\n",
    "    \n",
    "    num_edges = adj_coo.nnz\n",
    "    nnf = 1  ## nnf: num_node_features\n",
    "\n",
    "    src, dst = np.array(adj_coo.row), np.array(adj_coo.col)\n",
    "    edge_attr = np.array(adj_coo.data.reshape(num_edges, -1))\n",
    "    mask = np.where(src > dst, True, False)\n",
    "    edge_index_L = np.array([src[mask], dst[mask]])\n",
    "    edge_attr_L = np.array(edge_attr[mask])    \n",
    "    edge_index = np.concatenate((edge_index_L, edge_index_L[[1, 0]]), axis=1)\n",
    "    edge_attr = np.concatenate((edge_attr_L, edge_attr_L), axis=0)\n",
    "\n",
    "    data = Data(\n",
    "        x=torch.ones((num_beads, nnf)).float(),\n",
    "        edge_index=torch.tensor(edge_index, dtype=torch.long),\n",
    "        edge_attr=torch.tensor(edge_attr).float(),\n",
    "        y=torch.tensor(class_, dtype=torch.long),\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KrYnvRj-KHPF"
   },
   "source": [
    "## ネットワークの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOPTkv2urfVk"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #TODO: 入力グラフのチャネル数への依存をなくす\n",
    "        self.conv1 = GCNConv(1, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.conv3 = GCNConv(32, 48)\n",
    "        self.conv4 = GCNConv(48, 64)\n",
    "        self.conv5 = GCNConv(64, 96)\n",
    "        self.conv6 = GCNConv(96, 128)\n",
    "        self.linear1 = torch.nn.Linear(128,64)\n",
    "        #TODO: 分類数への依存をなくす\n",
    "        self.linear2 = torch.nn.Linear(64,9)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        #edge_weightとして用いるedge_attrのshapeが(n,1)だとうまくいかない．\n",
    "        #edge_attr.shapeが(n,)だと動く．おそらくPyG側のバグ？\n",
    "        #edge_weight = data.edge_attr\n",
    "        edge_weight = torch.squeeze(data.edge_attr)\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = scatter(x, data.batch, dim=0, reduce=\"max\")\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "adj_path = osp.join(root, \"raw\", \"adjMats.mtx\")\n",
    "class_path = osp.join(root, \"raw\", \"classes.mtx\")\n",
    "\n",
    "print(\"Reading graphs from {}\".format(root))\n",
    "\n",
    "adj_coo_mats = mmread(adj_path)\n",
    "classes = mmread(class_path)\n",
    "num_samples = adj_coo_mats.shape[0]\n",
    "data_list = []\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "print(\"Finish reading graphs from storage.\")\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))\n",
    "print(\"num_samples: {}\".format(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "print(\"Generating Data object\")\n",
    "\n",
    "data_list = Parallel(n_jobs=n_jobs)([delayed(load_graphs)(\n",
    "        adj_coo_mats=adj_coo_mats,\n",
    "        classes=classes,\n",
    "        i=i\n",
    "    ) for i in range(num_samples)])\n",
    "\n",
    "print(\"Post-processing\")       \n",
    "\n",
    "if add_noise:\n",
    "    print(\"Dropping edges\")\n",
    "    \n",
    "    data_list = Parallel(n_jobs=n_jobs)([delayed(drop_edge)(\n",
    "        data=data,\n",
    "        prob=drop_probability\n",
    "    ) for data in data_list])\n",
    "\n",
    "    print(\"Adding noise\")\n",
    "\n",
    "    data_list = Parallel(n_jobs=n_jobs)([delayed(multiply_lognormal_noise)(\n",
    "        data=data,\n",
    "        mean=mean,\n",
    "        sigma=sigma\n",
    "    ) for data in data_list])\n",
    "\n",
    "print(\"Inverting edge_attr\")\n",
    "\n",
    "data_list = Parallel(n_jobs=n_jobs)([delayed(invert_edge_attr)(\n",
    "        data=data,\n",
    "        pow_=pow_\n",
    "    ) for data in data_list])\n",
    "\n",
    "print(\"Finish post-processing!\")\n",
    "\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "val_size = round(num_samples * val_ratio)\n",
    "test_size = round(num_samples * test_ratio)\n",
    "train_size = num_samples - val_size - test_size\n",
    "cums = np.array([train_size, val_size, test_size]).cumsum()\n",
    "random.shuffle(data_list)\n",
    "\n",
    "train_data = data_list[0:cums[0]]\n",
    "val_data = data_list[cums[0]:cums[1]]\n",
    "test_data = data_list[cums[1]:cums[2]]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "nBeads = train_data[0].num_nodes\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0RaAOKXP5MK"
   },
   "source": [
    "## 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 192239,
     "status": "ok",
     "timestamp": 1594711504547,
     "user": {
      "displayName": "Hayato Onoue",
      "photoUrl": "",
      "userId": "07486968416296706504"
     },
     "user_tz": -540
    },
    "id": "A2IH_RUHPo7E",
    "outputId": "629436ed-26e4-4a53-e6ae-06082b38b2f4"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epoch_num):\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)\n",
    "        loss = criterion(outputs, batch.y - 1)  ## 1th indexed -> 0th indexed\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = torch.max(outputs, 1)[1] + 1  ## 0th indexed -> 1th indexed\n",
    "        train_total += batch.y.size(0)\n",
    "        train_correct += (predicted == batch.y).sum().cpu().item()\n",
    "\n",
    "        train_loss += loss.cpu().item()\n",
    "        if i % 10 == 9:\n",
    "            progress_bar = '['+('='*((i+1)//10))+(' '*((train_size//100-(i+1))//10))+']'\n",
    "            print('\\repoch: {:d} loss: {:.3f}  {}'\n",
    "                .format(\n",
    "                    epoch + 1,\n",
    "                    loss.cpu().item(),\n",
    "                    progress_bar),\n",
    "                end=\"  \")\n",
    "\n",
    "    train_acc = float(train_correct/train_total)\n",
    "\n",
    "    print('\\repoch: {:d} loss: {:.3f}'\n",
    "        .format(epoch + 1, train_loss / (train_size / batch_size)), end=\"  \")\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"train_loss\"].append(train_loss / (train_size / batch_size))\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_num = 0\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            loss += criterion(outputs,data.y - 1)  ## 1th indexed -> 0th indexed\n",
    "            predicted = torch.max(outputs, 1)[1] + 1  ## 0th indexed -> 1th indexed\n",
    "            total += data.y.size(0)\n",
    "            batch_num += 1\n",
    "            correct += (predicted == data.y).sum().cpu().item()\n",
    "\n",
    "    history[\"val_acc\"].append(correct/total)\n",
    "    history[\"val_loss\"].append(loss.cpu().item()/batch_num)\n",
    "    endstr = ' '*max(1,(train_size//1000-39))+\"\\n\"\n",
    "    print('Val Accuracy: {:.2f} %'.format(100 * float(correct/total)), end='  ')\n",
    "    print(f'Val Loss: {loss.cpu().item()/batch_num:.3f}',end=endstr)\n",
    "\n",
    "print('Finished Training')\n",
    "elapsed_time = time.time() - start\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))\n",
    "\n",
    "\n",
    "#テストデータでの最終結果出力\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        outputs = model(data)\n",
    "        predicted = torch.max(outputs, 1)[1] + 1  ## 0th indexed -> 1th indexed\n",
    "        total += data.y.size(0)\n",
    "        correct += (predicted == data.y).sum().cpu().item()\n",
    "print('Test Accuracy: {:.2f} %'.format(100 * float(correct/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yq4VX3bHXOJq"
   },
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 192227,
     "status": "ok",
     "timestamp": 1594711504548,
     "user": {
      "displayName": "Hayato Onoue",
      "photoUrl": "",
      "userId": "07486968416296706504"
     },
     "user_tz": -540
    },
    "id": "OfGA4f0IAMjT",
    "outputId": "36294f38-40f3-4f99-be3f-7595de6a9dc7"
   },
   "outputs": [],
   "source": [
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "\n",
    "x = np.arange(epoch_num) + 1\n",
    "plt.plot(x, history[\"train_acc\"], label=\"train acc\")\n",
    "plt.plot(x, history[\"val_acc\"], label=\"val acc\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験条件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_smaples = len(train_data)\n",
    "t = 20\n",
    "\n",
    "print(\"=====Simulation conditions=====\")\n",
    "print(\"目的：隣接町列を生成した数字の予測\")\n",
    "print(\"ネットワーク：GCNN\")\n",
    "print(\"Test run: {}\".format(test_run))\n",
    "print(\"Number of beads: {}\".format(nBeads))\n",
    "print(\"Number of samples for training: {}\".format(n_train_smaples))\n",
    "print(\"Add noise: {}\".format(add_noise))\n",
    "print(\"Number of epochs: {}\".format(epoch_num))\n",
    "print(\"Batch size: {}\".format(batch_size))\n",
    "print(\"Diffusion time: {}\".format(t))\n",
    "\n",
    "print(\"=====Results=====\")\n",
    "print(\"elapsed time: {}\".format(datetime.timedelta(seconds=int(elapsed_time))))\n",
    "print('Test Accuracy: {:.2f} %'.format(100 * float(correct/total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "adj2Digit_GCN_PyG.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
